"""
ë°ì´í„° ìˆ˜ì§‘ (Ingestion) ëª¨ë“ˆ
Bronze Layerë¡œ ì›ë³¸ ë°ì´í„° ìˆ˜ì§‘

ë°ì´í„° ì†ŒìŠ¤:
- KOSIS API: êµ­ê°€í†µê³„í¬í„¸
- ì •ì±… ë¬¸ì„œ: ìˆ˜ë™ ì¶”ì¶œ
- ì–¸ë¡  ë³´ë„: ìˆ˜ë™ ì¶”ì¶œ
"""

import requests
import json
import re
import time
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, List, Any
import pandas as pd

from config import (
    BRONZE_LAYER,
    DATA_SOURCES,
    init_data_lake
)
from warehouse import DataWarehouse


class KOSISIngestion:
    """KOSIS API ë°ì´í„° ìˆ˜ì§‘ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.config = DATA_SOURCES["kosis"]
        self.api_key = self.config["api_key"]
        self.base_url = self.config["base_url"]
    
    def _clean_json(self, raw_text: str) -> Optional[Dict]:
        """KOSIS JSON ì‘ë‹µ ì •ë¦¬ (ë”°ì˜´í‘œ ì—†ëŠ” í‚¤ ì²˜ë¦¬)"""
        try:
            # ë”°ì˜´í‘œ ì—†ëŠ” í‚¤ì— ë”°ì˜´í‘œ ì¶”ê°€
            fixed_text = re.sub(
                r'([{,])\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*:', 
                r'\1"\2":', 
                raw_text
            )
            return json.loads(fixed_text)
        except:
            return None
    
    def fetch_table(self, org_id: str, tbl_id: str, 
                    itm_id: str = "ALL", obj_l1: str = "ALL",
                    prd_cnt: int = 10) -> Optional[pd.DataFrame]:
        """KOSIS í†µê³„í‘œ ë°ì´í„° ìˆ˜ì§‘"""
        
        endpoint = f"{self.base_url}/Param/statisticsParameterData.do"
        
        params = {
            "method": "getList",
            "apiKey": self.api_key,
            "itmId": itm_id,
            "objL1": obj_l1,
            "format": "json",
            "jsonVD": "Y",
            "prdSe": "Y",
            "newEstPrdCnt": str(prd_cnt),
            "orgId": org_id,
            "tblId": tbl_id
        }
        
        try:
            query_string = "&".join([f"{k}={v}" for k, v in params.items()])
            full_url = f"{endpoint}?{query_string}"
            
            print(f"ğŸ“¡ Fetching: {tbl_id}")
            
            response = requests.get(full_url, timeout=30)
            data = self._clean_json(response.text)
            
            if data and isinstance(data, list):
                df = pd.DataFrame(data)
                df.columns = [col.upper() for col in df.columns]
                print(f"âœ… Fetched {len(df)} rows from {tbl_id}")
                return df
            elif data and 'err' in str(data):
                print(f"âš ï¸ API Error: {data}")
                return None
            else:
                return None
                
        except Exception as e:
            print(f"âŒ Fetch failed: {e}")
            return None
    
    def save_to_bronze(self, tbl_id: str, data: pd.DataFrame, 
                       source_url: str = "") -> Path:
        """Bronze ë ˆì´ì–´ì— ì›ë³¸ ë°ì´í„° ì €ì¥ (Parquet + JSON)"""
        
        output_dir = BRONZE_LAYER / "kosis"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Parquet ì €ì¥ (ë¶„ì„ìš©)
        parquet_path = output_dir / f"{tbl_id}_{timestamp}.parquet"
        data.to_parquet(parquet_path, index=False)
        
        # JSON ì €ì¥ (ë©”íƒ€ë°ì´í„° í¬í•¨)
        json_path = output_dir / f"{tbl_id}_{timestamp}.json"
        metadata = {
            "table_id": tbl_id,
            "ingested_at": datetime.now().isoformat(),
            "source_url": source_url,
            "row_count": len(data),
            "columns": list(data.columns),
            "data": data.to_dict(orient="records")
        }
        
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Saved to Bronze: {parquet_path.name}")
        
        return parquet_path
    
    def ingest_table(self, org_id: str, tbl_id: str, **kwargs) -> bool:
        """ë‹¨ì¼ í…Œì´ë¸” ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸"""
        
        # ë°ì´í„° ìˆ˜ì§‘
        df = self.fetch_table(org_id, tbl_id, **kwargs)
        
        if df is None or len(df) == 0:
            return False
        
        # Bronze ì €ì¥
        self.save_to_bronze(tbl_id, df)
        
        # ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì—ë„ ì €ì¥
        with DataWarehouse() as dw:
            dw.insert_bronze_data("raw_kosis_data", {
                "source_id": f"kosis_{tbl_id}",
                "table_id": tbl_id,
                "raw_data": df.to_dict(orient="records"),
                "source_url": f"{self.base_url}/Param/statisticsParameterData.do"
            })
            
            dw.log_pipeline_run(
                pipeline_name=f"kosis_ingestion_{tbl_id}",
                layer="bronze",
                status="success",
                rows_processed=len(df)
            )
        
        return True


class PolicyDocumentIngestion:
    """ì •ì±… ë¬¸ì„œ ë°ì´í„° ìˆ˜ì§‘ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.config = DATA_SOURCES["policy_documents"]
    
    def ingest_manual_data(self, document_id: str, document_name: str,
                           data: Dict[str, Any], source_path: str = "") -> bool:
        """ìˆ˜ë™ ì¶”ì¶œ ë°ì´í„° ìˆ˜ì§‘"""
        
        output_dir = BRONZE_LAYER / "policy_documents"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON ì €ì¥
        json_path = output_dir / f"{document_id}_{timestamp}.json"
        metadata = {
            "document_id": document_id,
            "document_name": document_name,
            "extracted_at": datetime.now().isoformat(),
            "source_path": source_path,
            "data": data
        }
        
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Saved policy document to Bronze: {json_path.name}")
        
        # ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì—ë„ ì €ì¥
        with DataWarehouse() as dw:
            dw.insert_bronze_data("raw_policy_data", {
                "document_id": document_id,
                "document_name": document_name,
                "raw_content": json.dumps(data, ensure_ascii=False),
                "source_path": source_path
            })
            
            dw.log_pipeline_run(
                pipeline_name=f"policy_ingestion_{document_id}",
                layer="bronze",
                status="success",
                rows_processed=1
            )
        
        return True


def ingest_ansan_policy_data():
    """ì•ˆì‚°ì‹œ ì •ì±… ë¬¸ì„œ ë°ì´í„° ìˆ˜ì§‘"""
    
    ingestion = PolicyDocumentIngestion()
    
    # ì •ì±… ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ ë°ì´í„°
    policy_data = {
        "policy_name": "ì™¸êµ­ì¸ê°€ì •ì˜ ì•ˆì „í•œ ë³´ìœ¡í™˜ê²½ ì¡°ì„±",
        "document_number": "3-3-48",
        "department": "ì•ˆì‚°ì‹œ ì—¬ì„±ë³´ìœ¡ê³¼",
        "contact": "031-481-3323",
        
        "support_target": "ê´€ë‚´ ì–´ë¦°ì´ì§‘ ì¬ì› ë“±ë¡ì™¸êµ­ì¸ ì•„ë™ (0~5ì„¸)",
        "residence_requirement": "ì•„ë™+ë³´í˜¸ì(1ëª…) ê²½ê¸°ë„ ë° ì•ˆì‚°ì‹œ 90ì¼ ì´ˆê³¼ ê±°ì£¼",
        
        "support_amounts": {
            "age_0_2": {"dobi": 100000, "sibi": 160000, "total": 260000},
            "age_3_5": {"dobi": 100000, "sibi": 180000, "total": 280000}
        },
        
        "extended_care": {
            "age_0": 3000,
            "age_1_2": 2000,
            "age_3_5": 1000
        },
        
        "performance_2024": {
            "childcare_recipients": 2144,
            "extended_care_recipients": 1434,
            "childcare_spent": 3938000000,
            "extended_care_spent": 188000000,
            "reference_date": "2024-08-31"
        },
        
        "budget_2025": {
            "total": 7284000000,
            "dobi": 1056000000,
            "sibi": 6228000000
        }
    }
    
    ingestion.ingest_manual_data(
        document_id="ansan_childcare_policy_2025",
        document_name="ì™¸êµ­ì¸ ë³´ìœ¡ë£Œ ì§€ì›(ì•ˆì‚°ì‹œ ì •ì±…).pdf",
        data=policy_data,
        source_path="ì™¸êµ­ì¸ ë³´ìœ¡ë£Œ ì§€ì›(ì•ˆì‚°ì‹œ ì •ì±…).pdf"
    )
    
    return True


def ingest_foreigner_statistics():
    """ì™¸êµ­ì¸ í˜„í™© í†µê³„ ë°ì´í„° ìˆ˜ì§‘"""
    
    ingestion = PolicyDocumentIngestion()
    
    # ì™¸êµ­ì¸ í˜„í™© ë°ì´í„° (ê°ì¢… ì¶œì²˜ì—ì„œ ìˆ˜ì§‘)
    statistics_data = {
        "ansan_foreigner_trend": [
            {"year": 2018, "total_population": 705000, "foreign_population": 78500, "ratio": 11.1},
            {"year": 2019, "total_population": 710000, "foreign_population": 82000, "ratio": 11.5},
            {"year": 2020, "total_population": 715000, "foreign_population": 79000, "ratio": 11.0},
            {"year": 2021, "total_population": 718000, "foreign_population": 85000, "ratio": 11.8},
            {"year": 2022, "total_population": 722000, "foreign_population": 90000, "ratio": 12.5},
            {"year": 2023, "total_population": 726000, "foreign_population": 93500, "ratio": 12.9},
            {"year": 2024, "total_population": 730000, "foreign_population": 96300, "ratio": 13.2}
        ],
        
        "gyeonggi_comparison": [
            {"region": "ì•ˆì‚°ì‹œ", "foreign_ratio": 13.2, "foreign_count": 96300, "pilot_program": False},
            {"region": "ì‹œí¥ì‹œ", "foreign_ratio": 10.1, "foreign_count": 47500, "pilot_program": False},
            {"region": "í™”ì„±ì‹œ", "foreign_ratio": 7.0, "foreign_count": 63000, "pilot_program": True},
            {"region": "ìˆ˜ì›ì‹œ", "foreign_ratio": 4.8, "foreign_count": 58000, "pilot_program": False},
            {"region": "ì•ˆì„±ì‹œ", "foreign_ratio": 5.0, "foreign_count": 9500, "pilot_program": True},
            {"region": "ì´ì²œì‹œ", "foreign_ratio": 4.0, "foreign_count": 8800, "pilot_program": True}
        ],
        
        "wongok_multicultural": {
            "total_residents": 20191,
            "foreign_residents": 18014,
            "korean_residents": 2177,
            "foreign_ratio": 89.2,
            "wongok_elementary": {
                "total_students": 449,
                "immigrant_background": 443,
                "ratio": 98.6
            }
        },
        
        "unregistered_children": {
            "moj_official_2025": 6169,
            "civil_society_low": 10000,
            "civil_society_high": 20000,
            "ansan_estimate_min": 814,
            "ansan_estimate_max": 2640,
            "ansan_estimate_mid": 1700
        },
        
        "sources": [
            {"name": "ì•ˆì‚°ì‹œì²­", "date": "2024-01", "type": "official"},
            {"name": "ë²•ë¬´ë¶€ ì¶œì…êµ­í†µê³„", "date": "2025-01", "type": "official"},
            {"name": "ê²½ê¸°ë„ì²­ ë³´ë„ìë£Œ", "date": "2025-12", "type": "official"},
            {"name": "ê²½ì¸ì¼ë³´", "date": "2024-01", "type": "news"},
            {"name": "ë™ì•„ì¼ë³´", "date": "2024-01", "type": "news"}
        ]
    }
    
    ingestion.ingest_manual_data(
        document_id="foreigner_statistics_2024",
        document_name="ì™¸êµ­ì¸ í˜„í™© í†µê³„ ì¢…í•©",
        data=statistics_data,
        source_path="multiple_sources"
    )
    
    return True


def run_full_ingestion():
    """ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    
    print("=" * 60)
    print("ğŸ“¥ Bronze Layer ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    print("=" * 60)
    
    # ë°ì´í„° ë ˆì´í¬ ì´ˆê¸°í™”
    init_data_lake()
    
    # ì •ì±… ë¬¸ì„œ ë°ì´í„° ìˆ˜ì§‘
    print("\nğŸ“„ ì •ì±… ë¬¸ì„œ ë°ì´í„° ìˆ˜ì§‘...")
    ingest_ansan_policy_data()
    
    # ì™¸êµ­ì¸ í˜„í™© í†µê³„ ìˆ˜ì§‘
    print("\nğŸ“Š ì™¸êµ­ì¸ í˜„í™© í†µê³„ ìˆ˜ì§‘...")
    ingest_foreigner_statistics()
    
    print("\n" + "=" * 60)
    print("âœ… Bronze Layer ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
    print("=" * 60)
    
    return True


if __name__ == "__main__":
    run_full_ingestion()
